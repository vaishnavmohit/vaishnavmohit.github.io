<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Mohit  Vaishnav | Experience</title>
<meta name="description" content="Professional experience spanning AI entrepreneurship, research, and industry. CEO & Co-founder at Kimova AI  (Google Cloud Startup Program), Postdoctoral Researcher at TalTech University, Data Scientist at Sandvik,  Researcher at DFKI (German AI Research Center), and PhD Researcher at Brown University. Expertise in machine learning,  computer vision, cybersecurity AI, automotive AI, and enterprise solutions across 4+ countries.">
<meta name="keywords" content="AI Experience, CEO Kimova AI, Google Cloud Startup, Postdoctoral Research, TalTech University,  Data Scientist, Sandvik AI, DFKI Researcher, Brown University Research, Computer Vision Expert,  Cybersecurity AI, ISO 27001 Automation, Machine Learning Industry, AI Entrepreneurship,  Academic Research, Industrial AI, Automotive AI, Healthcare AI, Mohit Vaishnav, AI Entrepreneur, CEO Kimova AI, Cybersecurity AI, ISO 27001 Compliance, Computer Vision, Deep Learning,  Visual Reasoning, PhD Artificial Intelligence, Brown University, ANITI, Cognitive AI, Transformer Architecture,  Machine Learning Expert, AI Research, Enterprise AI, Google Cloud Startup, NeurIPS, ICLR, Kaggle Competition Winner,  DFKI Researcher, Data Scientist, AI CEO, Technology Leadership, Innovation
">
<meta name="author" content="Dr. Mohit Vaishnav Vaishnav">
<meta name="robots" content="index, follow">

<!-- Additional SEO Meta Tags -->
<meta name="language" content="en">
<meta name="revisit-after" content="7 days">
<meta name="classification" content="Artificial Intelligence, Machine Learning, Computer Vision, Research">
<link rel="canonical" href="https://vaishnavmohit.github.io/experience/">

<!-- Twitter Card Meta Tags -->
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@mohitvaishnav">
<meta name="twitter:creator" content="@mohitvaishnav">
<meta name="twitter:title" content="Experience | Mohit Vaishnav">
<meta name="twitter:description" content="Professional experience spanning AI entrepreneurship, research, and industry. CEO & Co-founder at Kimova AI  (Google Cloud Startup Program), Postdoctoral Researcher at TalTech University, Data Scientist at Sandvik,  Researcher at DFKI (German AI Research Center), and PhD Researcher at Brown University. Expertise in machine learning,  computer vision, cybersecurity AI, automotive AI, and enterprise solutions across 4+ countries.">
<meta name="twitter:image" content="https://vaishnavmohit.github.io/assets/img/Kimova_Icon_small.png">
<!-- Schema.org Person JSON-LD for Google Knowledge Panel -->
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Person",
  "name": "Mohit Vaishnav",
  "image": "https://vaishnavmohit.github.io/assets/img/mohit_vaishnav.jpg",
  "jobTitle": "AI Entrepreneur & Researcher",
  "worksFor": {
    "@type": "Organization",
    "name": "Kimova AI",
    "url": "https://kimova.ai"
  },
  "alumniOf": [
    {
      "@type": "CollegeOrUniversity",
      "name": "Taltech University",
      "url": "https://taltech.ee/en"
    },
    {
      "@type": "CollegeOrUniversity",
      "name": "Brown University",
      "url": "https://brown.edu"
    },
    {
      "@type": "CollegeOrUniversity",
      "name": "ANITI",
      "url": "https://aniti.univ-toulouse.fr/"
    },
    {
      "@type": "CollegeOrUniversity",
      "name": "University of Toulouse III - Paul Sabatier",
      "url": "https://www.univ-tlse3.fr/home"
    },
    {
      "@type": "CollegeOrUniversity",
      "name": "Université Bourgogne Franche-Comté",
      "url": "https://www.u-bourgogne.fr/"
    },
    {
      "@type": "CollegeOrUniversity",
      "name": "Heriot-Watt University",
      "url": "https://www.hw.ac.uk/"
    },
    {
      "@type": "CollegeOrUniversity",
      "name": "LNM Institute of Information Technology",
      "url": "https://www.lnmiit.ac.in/"
    }
  ],
  "description": "Co-founder at Kimova AI (AI for ISO Cybersecurity Compliance). PhD in Cognitive AI & Visual Reasoning (Brown/ANITI). Bridging research with real-world impact.",
  "sameAs": [
  "https://github.com/vaishnavmohit",
  "https://twitter.com/mohitvaishnav",
  "https://www.linkedin.com/in/mohit-vaishnav",
  "https://scholar.google.com/citations?user=jGOzdbgAAAAJ&hl",
  "https://www.researchgate.net/profile/Mohit-Vaishnav-2",
  "https://serre-lab.clps.brown.edu/person/mohit-vaishnav/",
  "https://orcid.org/0000-0002-9795-493X",
  "https://www.etis.ee/CV/mohit-vaishnav/eng/",
  "https://openreview.net/profile?id=~Mohit_Vaishnav1",
  "https://cv.hal.science/mohitvaishnav",
  "https://dblp.org/pid/82/9414.html"
  ],
  "url": "https://vaishnavmohit.github.io/"
}
</script>

<!-- Open Graph Meta Tags -->

<meta property="og:site_name" content="Mohit Vaishnav - AI Entrepreneur & Research Leader | CEO at Kimova AI | PhD in Cognitive AI" />
<meta property="og:type" content="website" />
<meta property="og:title" content="Experience | Mohit Vaishnav" />
<meta property="og:url" content="https://vaishnavmohit.github.io/experience/" />
<meta property="og:description" content="Professional experience spanning AI entrepreneurship, research, and industry. CEO & Co-founder at Kimova AI  (Google Cloud Startup Program), Postdoctoral Researcher at TalTech University, Data Scientist at Sandvik,  Researcher at DFKI (German AI Research Center), and PhD Researcher at Brown University. Expertise in machine learning,  computer vision, cybersecurity AI, automotive AI, and enterprise solutions across 4+ countries." />
<meta property="og:image" content="https://vaishnavmohit.github.io/assets/img/Kimova_Icon_small.png" />
<meta property="og:image:alt" content="Mohit Vaishnav" />
<meta property="og:locale" content="en_US" />




<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<!-- Theming-->

  <script src="/assets/js/theme.js"></script>



  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-CJ1FLXGREG"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-CJ1FLXGREG');
  </script>
<!-- Schema.org ScholarlyArticle JSON-LD for Publications -->

<script type="application/ld+json">
[  {    "@context": "https://schema.org",    "@type": "ScholarlyArticle",    "name": "PhD Thesis: Exploring the role of (self-)attention in cognitive and computer vision architecture",    "author": {      "@type": "Person",      "name": "Mohit Vaishnav"    },    "datePublished": "2023",    "url": "https://arxiv.org/abs/2306.14650",    "description": "We investigate the role of attention and memory in complex reasoning tasks. We analyze Transformer-based self-attention as a model and extend it with memory. By studying a synthetic visual reasoning test, we refine the taxonomy of reasoning tasks. Incorporating self-attention with ResNet50, we enhance feature maps using feature-based and spatial attention, achieving efficient solving of challenging visual reasoning tasks. Our findings contribute to understanding the attentional needs of SVRT tasks. Additionally, we propose GAMR, a cognitive architecture combining attention and memory, inspired by active vision theory. GAMR outperforms other architectures in sample efficiency, robustness, and compositionality, and shows zero-shot generalization on new reasoning tasks."  },  {    "@context": "https://schema.org",    "@type": "ScholarlyArticle",    "name": "GAMR: Guided Attention Model for (visual) Reasoning",    "author": {      "@type": "Person",      "name": "Mohit Vaishnav"    },    "datePublished": "2023",    "url": "https://openreview.net/forum?id=iLMgk2IGNyv",    "description": "Humans continue to outperform modern AI systems in their ability to flexibly parse and understand complex visual scenes. Here, we present a novel module for visual reasoning, the Guided Attention Model for (visual) Reasoning (GAMR), which instantiates an active vision theory -- positing that the brain solves complex visual reasoning problems dynamically -- via sequences of attention shifts to select and route task-relevant visual information into memory. Experiments on an array of visual reasoning tasks and datasets demonstrate GAMR's ability to learn visual routines in a robust and sample-efficient manner. In addition, GAMR is shown to be capable of zero-shot generalization on completely novel reasoning tasks. Overall, our work provides computational support for cognitive theories that postulate the need for a critical interplay between attention and memory to dynamically maintain and manipulate task-relevant visual information to solve complex visual reasoning tasks."  },  {    "@context": "https://schema.org",    "@type": "ScholarlyArticle",    "name": "Understanding the Computational Demands Underlying Visual Reasoning",    "author": {      "@type": "Person",      "name": "Mohit Vaishnav"    },    "datePublished": "2022",    "url": "https://doi.org/10.1162/neco_a_01485",    "description": "Visual understanding requires comprehending complex visual relations between objects within a scene. Here, we seek to characterize the computational demands for abstract visual reasoning. We do this by systematically assessing the ability of modern deep convolutional neural networks (CNNs) to learn to solve the synthetic visual reasoning test (SVRT) challenge, a collection of 23 visual reasoning problems. Our analysis reveals a novel taxonomy of visual reasoning tasks, which can be primarily explained by both the type of relations (same-different versus spatial-relation judgments) and the number of relations used to compose the underlying rules. Prior cognitive neuroscience work suggests that attention plays a key role in humans' visual reasoning ability. To test this hypothesis, we extended the CNNs with spatial and feature-based attention mechanisms. In a second series of experiments, we evaluated the ability of these attention networks to learn to solve the SVRT challenge and found the resulting architectures to be much more efficient at solving the hardest of these visual reasoning tasks. Most important, the corresponding improvements on individual tasks partially explained our novel taxonomy. Overall, this work provides a granular computational account of visual reasoning and yields testable neuroscience predictions regarding the differential need for feature-based versus spatial attention depending on the type of visual reasoning problem."  },  {    "@context": "https://schema.org",    "@type": "ScholarlyArticle",    "name": "Conviformers: Convolutionally guided Vision Transformer",    "author": {      "@type": "Person",      "name": "Mohit Vaishnav"    },    "datePublished": "2022",    "url": "https://arxiv.org/pdf/2208.08900",    "description": "Vision transformers are nowadays the de-facto preference for image classification tasks. There are two broad categories of classification tasks, fine-grained and coarse-grained. In fine-grained classification, the necessity is to discover subtle differences due to the high level of similarity between sub-classes. Such distinctions are often lost as we downscale the image to save the memory and computational cost associated with vision transformers (ViT). In this work, we present an in-depth analysis and describe the critical components for developing a system for the fine-grained categorization of plants from herbarium sheets. Our extensive experimental analysis indicated the need for a better augmentation technique and the ability of modern-day neural networks to handle higher dimensional images. We also introduce a convolutional transformer architecture called Conviformer which, unlike the popular Vision Transformer (ConViT), can handle higher resolution images without exploding memory and computational cost. We also introduce a novel, improved pre-processing technique called PreSizer to resize images better while preserving their original aspect ratios, which proved essential for classifying natural plants. With our simple yet effective approach, we achieved SoTA on Herbarium 202x and iNaturalist 2019 dataset."  },  {    "@context": "https://schema.org",    "@type": "ScholarlyArticle",    "name": "A Benchmark for Compositional Visual Reasoning",    "author": {      "@type": "Person",      "name": "Mohit Vaishnav"    },    "datePublished": "2022",    "url": "https://arxiv.org/pdf/2206.05379",    "description": "A fundamental component of human vision is our ability to parse complex visual scenes and judge the relations between their constituent objects. AI benchmarks for visual reasoning have driven rapid progress in recent years with state-of-the-art systems now reaching human accuracy on some of these benchmarks. Yet, a major gap remains in terms of the sample efficiency with which humans and AI systems learn new visual reasoning tasks. Humans' remarkable efficiency at learning has been at least partially attributed to their ability to harness compositionality -- such that they can efficiently take advantage of previously gained knowledge when learning new tasks. Here, we introduce a novel visual reasoning benchmark, Compositional Visual Relations (CVR), to drive progress towards the development of more data-efficient learning algorithms. We take inspiration from fluidic intelligence and non-verbal reasoning tests and describe a novel method for creating compositions of abstract rules and associated image datasets at scale. Our proposed benchmark includes measures of sample efficiency, generalization and transfer across task rules, as well as the ability to leverage compositionality. We systematically evaluate modern neural architectures and find that, surprisingly, convolutional architectures surpass transformer-based architectures across all performance measures in most data regimes. However, all computational models are a lot less data efficient compared to humans even after learning informative visual representations using self-supervision. Overall, we hope that our challenge will spur interest in the development of neural architectures that can learn to harness compositionality toward more efficient learning."  }]
</script>


    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="https://vaishnavmohit.github.io/">
       <span class="font-weight-bold">Mohit</span>   Vaishnav
      </a>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          
          <!-- Other pages -->
          
          
          
            
          
            
              <li class="nav-item ">
                <a class="nav-link" href="/experience_backup/">
                  
                  
                </a>
              </li>
            
          
            
          
            
          
            
          
            
          
            
          
            
              <li class="nav-item ">
                <a class="nav-link" href="/education/">
                  Education
                  
                </a>
              </li>
            
          
            
              <li class="nav-item active">
                <a class="nav-link" href="/experience/">
                  Experience
                  
                  <span class="sr-only">(current)</span>
                  
                </a>
              </li>
            
          
            
          
            
          
            
          
            
              <li class="nav-item ">
                <a class="nav-link" href="/awards/">
                  Recognition
                  
                </a>
              </li>
            
          
          <!-- Research Dropdown -->
          <li class="nav-item dropdown ">
  <a class="nav-link" href="#" id="navbarDropdownResearch">
    Research
  </a>
            <div class="dropdown-menu" aria-labelledby="navbarDropdownResearch">
              <a class="dropdown-item" href="/publications/">Publications</a>
              <a class="dropdown-item" href="/phd-thesis/">PhD Thesis</a>
            </div>
          </li>
    <!-- CV -->
          <li class="nav-item ">
      <a class="nav-link" target="_blank" href="/assets/pdf/Mohit_Vaishnav_cv.pdf">CV</a>
          </li>
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">


  <article>
    <!-- JSON-LD Structured Data for Professional Experience -->
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Person",
  "name": "Mohit Vaishnav",
  "hasOccupation": [
    {
      "@type": "Occupation",
      "name": "Chief Executive Officer and Co-founder",
      "occupationLocation": {
        "@type": "Organization",
        "name": "Kimova AI",
        "url": "https://kimova.ai",
        "description": "AI-powered ISO 27001 cybersecurity compliance automation"
      },
      "skills": ["Artificial Intelligence", "Machine Learning", "Cybersecurity", "ISO 27001", "Leadership", "Entrepreneurship", "Product Strategy"],
      "startDate": "2024",
      "responsibilities": "Leading AI-driven cybersecurity compliance innovation, secured Google Cloud Startup Program backing, spearheading strategic partnerships and client engagements"
    },
    {
      "@type": "Occupation",
      "name": "Postdoctoral Researcher",
      "occupationLocation": {
        "@type": "Organization",
        "name": "TalTech University",
        "address": {
          "@type": "PostalAddress",
          "addressCountry": "Estonia"
        }
      },
      "skills": ["Multimodal AI", "Large Language Models", "Vision-Language Models", "Research", "Deep Learning"],
      "startDate": "2024",
      "responsibilities": "Leading EXAI grant research on multimodal reasoning agents, designing evaluation pipelines for large vision-language models, establishing new benchmarks"
    },
    {
      "@type": "Occupation",
      "name": "Data Scientist",
      "occupationLocation": {
        "@type": "Organization",
        "name": "Sandvik GmbH - Center for AI and Automation",
        "address": {
          "@type": "PostalAddress",
          "addressCountry": "Germany"
        }
      },
      "skills": ["Azure AI", "Machine Learning", "Data Science", "AI Solutions", "Cloud Computing"],
      "startDate": "2024",
      "endDate": "2024",
      "responsibilities": "Led development and deployment of end-to-end AI solutions within Azure environment, collaborated with stakeholders on strategic initiatives"
    },
    {
      "@type": "Occupation",
      "name": "Researcher",
      "occupationLocation": {
        "@type": "Organization",
        "name": "German AI Research Center (DFKI)",
        "url": "https://www.dfki.de",
        "address": {
          "@type": "PostalAddress",
          "addressCountry": "Germany"
        }
      },
      "skills": ["Computer Vision", "Bayesian Networks", "Automotive AI", "Perception Systems", "EU Research"],
      "startDate": "2023",
      "endDate": "2023",
      "responsibilities": "Designed perception systems for driver behavioral modeling, applied Bayesian Belief Networks for scalable models, contributed to EU Horizon project BERTHA"
    },
    {
      "@type": "Occupation",
      "name": "Doctoral Researcher",
      "occupationLocation": {
        "@type": "Organization",
        "name": "Brown University - Serre Lab",
        "url": "https://serre-lab.clps.brown.edu",
        "address": {
          "@type": "PostalAddress",
          "addressLocality": "Providence",
          "addressRegion": "RI",
          "addressCountry": "USA"
        }
      },
      "skills": ["Deep Learning", "Computer Vision", "Visual Reasoning", "Transformer Architecture", "Research Publication", "HPC Computing"],
      "startDate": "2019",
      "endDate": "2023",
      "responsibilities": "Conducted research on abstract reasoning and attention mechanisms, developed novel architectures, published in NeurIPS, ICLR, and Neural Computation, optimized multi-million parameter models on HPC clusters"
    }
  ]
}
</script>

<div class="experience-container">
  
  <section class="experience-section">
    <h2 class="section-title">Professional Experience</h2>
    <div class="experience-timeline">
      <div class="experience-item current">
        <div class="experience-dot"></div>
        <div class="experience-card">
          <div class="experience-header">
            <div class="role-info">
              <h3>Co-founder</h3>
              <h4>Kimova AI</h4>
            </div>
            <span class="experience-period current-role">2024 - Present</span>
          </div>
          <p class="company-description">Pioneering ISO auditing company leveraging AI technology to transform cybersecurity and compliance practices.</p>
          <ul class="achievements">
            <li>Secured backing from the Google Cloud Startup Program to accelerate growth and enhance service offerings</li>
            <li>Revolutionizing ISO/IEC 27001:2022 compliance assessments with innovative solutions</li>
            <li>Spearheaded strategic partnerships and client engagements, driving business development</li>
          </ul>
          <div class="experience-tags">
            <span class="tag startup">Startup</span>
            <span class="tag ai">AI/ML</span>
            <span class="tag cybersecurity">Cybersecurity</span>
          </div>
        </div>
      </div>
      <div class="experience-item">
        <div class="experience-dot"></div>
        <div class="experience-card">
          <div class="experience-header">
            <div class="role-info">
              <h3>Postdoctoral Researcher</h3>
              <h4>TalTech University, Estonia</h4>
            </div>
            <span class="experience-period">2024 - Present</span>
          </div>
          <ul class="achievements">
            <li>Leading multidisciplinary research under the EXAI grant focusing on multimodal reasoning agents</li>
            <li>Designing novel evaluation pipelines for large vision-language models</li>
            <li>Developed AI agents demonstrating enhanced reasoning capabilities</li>
            <li>Established new state-of-the-art benchmarks on real-world datasets</li>
          </ul>
          <div class="experience-tags">
            <span class="tag research">Research</span>
            <span class="tag ai">AI/ML</span>
            <span class="tag academia">Academia</span>
          </div>
        </div>
      </div>
      <div class="experience-item">
        <div class="experience-dot"></div>
        <div class="experience-card">
          <div class="experience-header">
            <div class="role-info">
              <h3>Data Scientist</h3>
              <h4>Sandvik GmbH - Center for AI and Automation</h4>
            </div>
            <span class="experience-period">2024</span>
          </div>
          <ul class="achievements">
            <li>Led development and deployment of end-to-end AI solutions within Azure environment</li>
            <li>Collaborated with stakeholders to align business needs and execute strategic initiatives</li>
            <li>Delivered platform responsibility ensuring seamless integration of AI services</li>
          </ul>
          <div class="experience-tags">
            <span class="tag industry">Industry</span>
            <span class="tag azure">Azure</span>
            <span class="tag ai">AI/ML</span>
          </div>
        </div>
      </div>
      <div class="experience-item">
        <div class="experience-dot"></div>
        <div class="experience-card">
          <div class="experience-header">
            <div class="role-info">
              <h3>Researcher</h3>
              <h4>German AI Research Center (DFKI)</h4>
            </div>
            <span class="experience-period">2023</span>
          </div>
          <ul class="achievements">
            <li>Designed perception systems for driver behavioral modeling in CCAM projects</li>
            <li>Applied Bayesian Belief Networks for scalable Driver Behavioral Models</li>
            <li>Contributed to EU Horizon project BERTHA</li>
          </ul>
          <div class="experience-tags">
            <span class="tag research">Research</span>
            <span class="tag automotive">Automotive</span>
            <span class="tag eu-project">EU Project</span>
          </div>
        </div>
      </div>
      <div class="experience-item">
        <div class="experience-dot"></div>
        <div class="experience-card">
          <div class="experience-header">
            <div class="role-info">
              <h3>Doctoral Researcher</h3>
              <h4>Serre Lab, Brown University</h4>
            </div>
            <span class="experience-period">2019 - 2023</span>
          </div>
          <ul class="achievements">
            <li>Conducted research on abstract reasoning tasks and attention mechanisms</li>
            <li>Developed novel architectures combining attention and memory</li>
            <li>Published papers in NeurIPS, ICLR, and Neural Computation</li>
            <li>Optimized multi-million parameter Transformer models</li>
            <li>Worked with multi-GPU environments using Slurm on HPC clusters</li>
          </ul>
          <div class="experience-tags">
            <span class="tag phd">PhD</span>
            <span class="tag research">Research</span>
            <span class="tag publications">Publications</span>
          </div>
        </div>
      </div>
    </div>
  </section>
  <section class="experience-section">
    <h2 class="section-title">Entrepreneurial Experience</h2>
    <div class="experience-grid">
      <div class="experience-card small">
        <h3>Shree Bherunath Granite</h3>
        <p class="company-location">Rajsamad, India (2014-2016)</p>
        <p>Helped family setup granite mines and oversaw day-to-day operations supervising 20 employees.</p>
        <div class="experience-tags">
          <span class="tag business">Business</span>
          <span class="tag operations">Operations</span>
        </div>
      </div>
      <div class="experience-card small">
        <h3>Kevin Technology</h3>
        <p class="company-location">Ajmer, India (2016-2017)</p>
        <p>Co-founded a startup developing surveillance systems based on computer vision techniques.</p>
        <div class="experience-tags">
          <span class="tag startup">Startup</span>
          <span class="tag computer-vision">Computer Vision</span>
        </div>
      </div>
    </div>
  </section>

  <section class="experience-section">
    <h2 class="section-title">Industrial Experience</h2>
    <div class="experience-grid">
      <div class="experience-card small">
        <h3>Research Engineer</h3>
        <h4>WeDiagnostiX</h4>
        <p class="company-location">Feb 2019 - July 2019</p>
        <ul class="achievements small">
          <li>Developed first working prototype for maxillary structures classification from X-ray images</li>
          <li>Utilized Deep Learning for semantic and instance segmentation of teeth</li>
          <li>Created end-to-end pipeline from data collection to functional prototype</li>
        </ul>
        <div class="experience-tags">
          <span class="tag medical">Medical AI</span>
          <span class="tag computer-vision">Computer Vision</span>
        </div>
      </div>
      <div class="experience-card small">
        <h3>Research Engineer</h3>
        <h4>Quelia Systems Inc.</h4>
        <p class="company-location">July 2018 - Aug 2018</p>
        <ul class="achievements small">
          <li>Developed application for tire wear estimation using mobile camera</li>
          <li>Applied computer vision techniques for tread depth measurement</li>
          <li>Enhanced road safety through user-friendly tire assessment solution</li>
        </ul>
        <div class="experience-tags">
          <span class="tag automotive">Automotive</span>
          <span class="tag computer-vision">Computer Vision</span>
        </div>
      </div>
    </div>
  </section>

  <section class="experience-section">
    <h2 class="section-title">Academic Experience</h2>
    <div class="academic-content">
      <div class="academic-item">
        <h3>Peer Review &amp; Editorial Work</h3>
        <p>Reviewer for prestigious journals and conferences including:</p>
        <ul class="academic-list">
          <li>IEEE Transactions on Evolutionary Computation (2012)</li>
          <li>NeurIPS (2021)</li>
          <li>CVPR (2022, 2023)</li>
          <li>ICML (2022)</li>
          <li>ECCV (2022)</li>
        </ul>
        <div class="experience-tags">
          <span class="tag research">Research</span>
          <span class="tag peer-review">Peer Review</span>
        </div>
      </div>
      <div class="academic-item">
        <h3>Research Assistant</h3>
        <h4>Ajman University, U.A.E.</h4>
        <p>Conducted research in computer vision and machine learning applications.</p>
        <div class="experience-tags">
          <span class="tag research">Research</span>
          <span class="tag international">International</span>
        </div>
      </div>
      <div class="academic-item">
        <h3>Offshore Research Collaboration</h3>
        <h4>Suspect Technologies (MIT Camera Culture Group)</h4>
        <p>Collaborated with startup founded by <strong>Massachusetts Institute of Technology</strong> (<a href="https://www.mit.edu/" target="_blank" class="institute-link">MIT</a>) Camera Culture Group members.</p>
        <div class="experience-tags">
          <span class="tag startup">Startup</span>
          <span class="tag mit">MIT</span>
          <span class="tag computer-vision">Computer Vision</span>
        </div>
      </div>
      <div class="academic-item">
        <h3>Kumbhathon Challenge 2015</h3>
        <h4><a href="https://www.kumbha.org/" target="_blank" class="institute-link">Kumbha Festival Innovation</a></h4>
        <p>Contributed to innovating Kumbha festival in collaboration with MIT USA, focusing on "Abnormal Motion Detection" technologies.</p>
        <div class="experience-tags">
          <span class="tag innovation">Innovation</span>
          <span class="tag computer-vision">Computer Vision</span>
          <span class="tag mit">MIT</span>
        </div>
      </div>
      <div class="academic-item">
        <h3>Summer Research Internship</h3>
        <h4><strong>Indian Institute of Science</strong> (<a href="https://iisc.ac.in/" target="_blank" class="institute-link">IISc</a>), Bangalore</h4>
        <p><strong>Advisor:</strong> <a href="http://iiscprofiles.irins.org/profile/3742" target="_blank" class="advisor-link">Prof. K R Ramakrishnan</a></p>
        <p><strong>Project:</strong> Authored comprehensive survey article on "3D Video Synopsis: Capturing to Transmission"</p>
        <div class="experience-tags">
          <span class="tag research">Research</span>
          <span class="tag iisc">IISc</span>
          <span class="tag 3d-vision">3D Vision</span>
        </div>
      </div>
      <div class="academic-item">
        <h3>Global Internship Program In Engineering Innovation And Design</h3>
        <h4><a href="http://gipedi.iitd.ac.in/" target="_blank" class="institute-link">GIPEDI</a>, <strong>Indian Institute of Technology</strong> (<a href="https://home.iitd.ac.in/" target="_blank" class="institute-link">IIT</a>) Delhi</h4>
        <p><strong>Advisor:</strong> <a href="https://web.iitd.ac.in/~subrat/" target="_blank" class="advisor-link">Prof. Subrat Kar</a></p>
        <p><strong>Project:</strong> Comprehensive review on Compression Sensing techniques and applications</p>
        <div class="experience-tags">
          <span class="tag research">Research</span>
          <span class="tag iit">IIT</span>
          <span class="tag signal-processing">Signal Processing</span>
        </div>
      </div>
      <div class="academic-item">
        <h3>Research Project</h3>
        <h4><strong>Indian Institute of Technology</strong> (<a href="https://www.iitj.ac.in/" target="_blank" class="institute-link">IIT</a>) Jodhpur</h4>
        <p><strong>Advisor:</strong> <a href="http://home.iitj.ac.in/~akt/" target="_blank" class="advisor-link">Prof. Anil Kumar Tiwari</a></p>
        <p><strong>Project:</strong> Developed innovative lossless video compression techniques and published findings in top-tier Data Compression Conference</p>
        <div class="experience-tags">
          <span class="tag research">Research</span>
          <span class="tag iit">IIT</span>
          <span class="tag publications">Publications</span>
        </div>
      </div>
    </div>
  </section>

  <section class="experience-section">
    <h2 class="section-title">Conference Talks</h2>
    <div class="academic-content">
      <div class="talk-item">
        <h3 class="talk-title">Using Artificial Intelligence To Identify Fossil Angiosperm Leaves At Family Level</h3>
        <p class="talk-authors">Ivan Felipe, Thomas FEL, <strong>Mohit Vaishnav</strong>, Peter Wilf, Thomas Serre</p>
        <p class="talk-venue"><strong>Geological Society of America</strong>, Connects, Denver (USA) 2022</p>
        <div class="experience-tags">
          <span class="tag ai">AI</span>
          <span class="tag paleontology">Paleontology</span>
          <span class="tag conference">Conference</span>
        </div>
      </div>
      <div class="talk-item">
        <h3 class="talk-title">Understanding how deep neural networks categorize living and fossil leaves</h3>
        <p class="talk-authors"><strong>Mohit Vaishnav</strong>, Thomas FEL, Ivan Felipe, Jacob A Rose, Peter Wilf, Thomas Serre</p>
        <p class="talk-venue"><strong>Botany</strong> (virtual) 2021</p>
        <div class="experience-tags">
          <span class="tag deep-learning">Deep Learning</span>
          <span class="tag botany">Botany</span>
          <span class="tag virtual">Virtual</span>
        </div>
      </div>
      <div class="talk-item">
        <h3 class="talk-title">A deep-learning-based approach for automated fossil leaf identification</h3>
        <p class="talk-authors">Ivan Felipe, Jacob A Rose, Thomas FEL, <strong>Mohit Vaishnav</strong>, Peter Wilf, Thomas Serre</p>
        <p class="talk-venue"><strong>Botany</strong> (virtual) 2021</p>
        <div class="experience-tags">
          <span class="tag deep-learning">Deep Learning</span>
          <span class="tag automation">Automation</span>
          <span class="tag botany">Botany</span>
        </div>
      </div>
      <div class="talk-item">
        <h3 class="talk-title">Computational models of visual reasoning</h3>
        <p class="talk-authors"><strong>Mohit Vaishnav</strong></p>
        <p class="talk-venue"><strong>Brown Unconference</strong>, 2021</p>
        <div class="experience-tags">
          <span class="tag visual-reasoning">Visual Reasoning</span>
          <span class="tag brown">Brown</span>
          <span class="tag computational">Computational</span>
        </div>
      </div>
    </div>
  </section>

  <section class="experience-section">
    <h2 class="section-title">Teaching</h2>
    <div class="academic-content">
      <div class="teaching-item">
        <h3>M1 Student Supervision</h3>
        <h4>Paul Sabastier University, France</h4>
        <p class="teaching-period">January - May 2021</p>
        <p>Supervised 15 M1 students for the course <strong>Initiation to research work (project) (EMINC2B2)</strong>, guiding them through their first research experiences.</p>
        <div class="experience-tags">
          <span class="tag teaching">Teaching</span>
          <span class="tag supervision">Supervision</span>
          <span class="tag france">France</span>
        </div>
      </div>
      <div class="teaching-item">
        <h3>Computer Vision Instructor</h3>
        <h4>Federal University of Toulouse Midi-Pyrénées, France</h4>
        <p class="teaching-period">March - May 2021, 2022</p>
        <p>Taught <strong><a href="https://rufinv.github.io/Intro2AI-advanced-class/" target="_blank" class="course-link">Basics of Introduction to Computer Vision</a></strong> course, covering fundamental concepts and advanced applications.</p>
        <div class="experience-tags">
          <span class="tag teaching">Teaching</span>
          <span class="tag computer-vision">Computer Vision</span>
          <span class="tag france">France</span>
        </div>
      </div>
      <div class="teaching-item">
        <h3>Visual Reasoning Instructor</h3>
        <h4>Federal University of Toulouse Midi-Pyrénées, France</h4>
        <p class="teaching-period">March - May 2021, 2022</p>
        <p>Designed and delivered <strong><a href="https://rufinv.github.io/Intro2AI-advanced-class/" target="_blank" class="course-link">Visual Reasoning in Computer Vision</a></strong> course, exploring advanced AI reasoning mechanisms.</p>
        <div class="experience-tags">
          <span class="tag teaching">Teaching</span>
          <span class="tag visual-reasoning">Visual Reasoning</span>
          <span class="tag ai">AI</span>
        </div>
      </div>
      <div class="teaching-item">
        <h3>Teaching Assistant</h3>
        <h4>The LNMIIT, Jaipur, India</h4>
        <p>Assisted in Electronics laboratory sessions, helping students with practical implementations and theoretical understanding.</p>
        <div class="experience-tags">
          <span class="tag teaching">Teaching</span>
          <span class="tag electronics">Electronics</span>
          <span class="tag lab">Laboratory</span>
        </div>
      </div>
    </div>
  </section>

  <section class="experience-section">
    <h2 class="section-title">Workshops &amp; Professional Development</h2>
    <div class="academic-content">
      <div class="workshop-item">
        <h3><a href="https://rlvs.aniti.fr/" target="_blank" class="workshop-link">Reinforcement Learning Virtual School</a></h3>
        <p class="workshop-organizer">Organized by <strong>ANITI</strong>, 2021</p>
        <p>Advanced training in reinforcement learning algorithms, theory, and applications in modern AI systems.</p>
        <div class="experience-tags">
          <span class="tag reinforcement-learning">RL</span>
          <span class="tag aniti">ANITI</span>
          <span class="tag virtual">Virtual</span>
        </div>
      </div>
      <div class="workshop-item">
        <h3><a href="https://www.coursera.org/learn/computational-neuroscience" target="_blank" class="workshop-link">Computational Neuroscience</a></h3>
        <p class="workshop-organizer">Coursera, 2021</p>
        <p>Comprehensive course covering mathematical models of brain function and neural computation principles.</p>
        <div class="experience-tags">
          <span class="tag neuroscience">Neuroscience</span>
          <span class="tag computational">Computational</span>
          <span class="tag coursera">Coursera</span>
        </div>
      </div>
      <div class="workshop-item">
        <h3><a href="https://www.translationalneuromodeling.org/cpcourse/" target="_blank" class="workshop-link">Computational Psychiatry Course</a></h3>
        <p class="workshop-organizer">Translational Neuromodeling Unit, University of Zurich &amp; ETH Zurich</p>
        <p class="workshop-period">September 2020</p>
        <p>Specialized training in computational approaches to understanding psychiatric disorders and mental health.</p>
        <div class="experience-tags">
          <span class="tag psychiatry">Psychiatry</span>
          <span class="tag computational">Computational</span>
          <span class="tag zurich">Zurich</span>
        </div>
      </div>
    </div>
  </section>

</div>

<style>
  .experience-container {
    max-width: 1000px;
    margin: 0 auto;
    padding: 2rem;
    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
  }

  .experience-section {
    margin-bottom: 4rem;
  }

  .section-title {
    color: #2c3e50;
    font-size: 2rem;
    font-weight: 700;
    margin-bottom: 2rem;
    position: relative;
    padding-bottom: 0.5rem;
  }

  html[data-theme='dark'] .section-title {
    color: var(--global-text-color);
  }

  .section-title::after {
    content: '';
    position: absolute;
    bottom: 0;
    left: 0;
    width: 60px;
    height: 3px;
    background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
  }

  html[data-theme='dark'] .section-title::after {
    background: linear-gradient(90deg, var(--global-theme-color) 0%, #764ba2 100%);
  }

  .experience-timeline {
    position: relative;
    padding-left: 2rem;
  }

  .experience-timeline::before {
    content: '';
    position: absolute;
    left: 15px;
    top: 0;
    bottom: 0;
    width: 2px;
    background: linear-gradient(180deg, #667eea 0%, #764ba2 100%);
  }

  html[data-theme='dark'] .experience-timeline::before {
    background: linear-gradient(180deg, var(--global-theme-color) 0%, #764ba2 100%);
  }

  .experience-item {
    position: relative;
    margin-bottom: 2.5rem;
  }

  .experience-item:last-child {
    margin-bottom: 0;
  }

  .experience-dot {
    position: absolute;
    left: -27px;
    top: 1rem;
    width: 14px;
    height: 14px;
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    border: 3px solid white;
    border-radius: 50%;
    box-shadow: 0 2px 8px rgba(102, 126, 234, 0.3);
    z-index: 2;
  }

  html[data-theme='dark'] .experience-dot {
    background: linear-gradient(135deg, var(--global-theme-color) 0%, #764ba2 100%);
    border: 3px solid var(--global-card-bg-color);
  }

  .experience-item.current .experience-dot {
    animation: pulse 2s infinite;
  }

  @keyframes pulse {
    0% { box-shadow: 0 2px 8px rgba(102, 126, 234, 0.3); }
    50% { box-shadow: 0 2px 8px rgba(102, 126, 234, 0.6), 0 0 20px rgba(102, 126, 234, 0.3); }
    100% { box-shadow: 0 2px 8px rgba(102, 126, 234, 0.3); }
  }

  .experience-card {
    background: linear-gradient(135deg, #ffffff 0%, #f8f9fa 100%);
    border: 1px solid #e9ecef;
    border-radius: 16px;
    padding: 2rem;
    box-shadow: 0 4px 20px rgba(0, 0, 0, 0.08);
    transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
    position: relative;
    overflow: hidden;
  }

  html[data-theme='dark'] .experience-card {
    background: linear-gradient(135deg, var(--global-card-bg-color) 0%, #2a2a2a 100%);
    border: 1px solid #333;
    box-shadow: 0 4px 20px rgba(0, 0, 0, 0.3);
  }

  .experience-card::before {
    content: '';
    position: absolute;
    top: 0;
    left: 0;
    right: 0;
    height: 4px;
    background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
  }

  .experience-card:hover {
    transform: translateY(-4px);
    box-shadow: 0 8px 30px rgba(102, 126, 234, 0.15);
  }

  html[data-theme='dark'] .experience-card:hover {
    box-shadow: 0 8px 30px rgba(0, 0, 0, 0.4);
  }

  .experience-header {
    display: flex;
    justify-content: space-between;
    align-items: flex-start;
    margin-bottom: 1rem;
    flex-wrap: wrap;
    gap: 1rem;
  }

  .role-info h3 {
    margin: 0 0 0.25rem 0;
    color: #2c3e50;
    font-size: 1.3rem;
    font-weight: 600;
  }

  html[data-theme='dark'] .role-info h3 {
    color: var(--global-text-color);
  }

  .role-info h4 {
    margin: 0;
    color: #667eea;
    font-size: 1.1rem;
    font-weight: 500;
  }

  html[data-theme='dark'] .role-info h4 {
    color: var(--global-theme-color);
  }

  .experience-period {
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    color: white;
    padding: 0.5rem 1rem;
    border-radius: 20px;
    font-size: 0.9rem;
    font-weight: 500;
    white-space: nowrap;
  }

  html[data-theme='dark'] .experience-period {
    background: linear-gradient(135deg, var(--global-theme-color) 0%, #764ba2 100%);
  }

  .current-role {
    background: linear-gradient(135deg, #27ae60 0%, #229954 100%);
    animation: glow 3s ease-in-out infinite alternate;
  }

  html[data-theme='dark'] .current-role {
    background: linear-gradient(135deg, #2ecc71 0%, #27ae60 100%);
  }

  @keyframes glow {
    from { box-shadow: 0 2px 8px rgba(39, 174, 96, 0.3); }
    to { box-shadow: 0 2px 8px rgba(39, 174, 96, 0.6), 0 0 20px rgba(39, 174, 96, 0.3); }
  }

  .company-description {
    color: #495057;
    font-style: italic;
    margin-bottom: 1rem;
    font-size: 1rem;
  }

  html[data-theme='dark'] .company-description {
    color: var(--global-text-color);
  }

  .company-location {
    color: #6c757d;
    font-size: 0.9rem;
    margin-bottom: 0.5rem;
  }

  html[data-theme='dark'] .company-location {
    color: #aaa;
  }

  .achievements {
    list-style: none;
    padding: 0;
    margin: 1rem 0;
  }

  .achievements li {
    position: relative;
    padding-left: 1.5rem;
    margin-bottom: 0.75rem;
    color: #495057;
    line-height: 1.5;
  }

  html[data-theme='dark'] .achievements li {
    color: var(--global-text-color);
  }

  .achievements li::before {
    content: '▸';
    position: absolute;
    left: 0;
    color: #667eea;
    font-weight: bold;
  }

  html[data-theme='dark'] .achievements li::before {
    color: var(--global-theme-color);
  }

  .achievements.small li {
    margin-bottom: 0.5rem;
    font-size: 0.9rem;
  }

  .experience-tags {
    /* display: flex; */
    flex-wrap: wrap;
    gap: 0.5rem;
    margin-top: 1rem;
  }

  .tag {
    padding: 0.25rem 0.75rem;
    border-radius: 12px;
    font-size: 0.75rem;
    font-weight: 500;
    text-transform: uppercase;
    letter-spacing: 0.5px;
  }

  .tag.startup { background: #fff3cd; color: #856404; }
  .tag.ai { background: #d4edda; color: #155724; }
  .tag.research { background: #cce5ff; color: #004085; }
  .tag.industry { background: #f8d7da; color: #721c24; }
  .tag.academia { background: #e2e3e5; color: #383d41; }
  .tag.phd { background: #d1ecf1; color: #0c5460; }
  .tag.publications { background: #fff3e0; color: #e65100; }
  .tag.cybersecurity { background: #fce4ec; color: #880e4f; }
  .tag.azure { background: #e3f2fd; color: #0d47a1; }
  .tag.automotive { background: #f3e5f5; color: #4a148c; }
  .tag.eu-project { background: #e8f5e8; color: #2e7d32; }
  .tag.business { background: #fff8e1; color: #ff8f00; }
  .tag.operations { background: #fafafa; color: #424242; }
  .tag.computer-vision { background: #e1f5fe; color: #006064; }
  .tag.medical { background: #e8f5e8; color: #1b5e20; }

  html[data-theme='dark'] .tag.startup { background: #4a3c00; color: #ffeb9c; }
  html[data-theme='dark'] .tag.ai { background: #0d3d1a; color: #c8e6c9; }
  html[data-theme='dark'] .tag.research { background: #1a237e; color: #bbdefb; }
  html[data-theme='dark'] .tag.industry { background: #4a1a1a; color: #ffcdd2; }
  html[data-theme='dark'] .tag.academia { background: #2e2e2e; color: #e0e0e0; }
  html[data-theme='dark'] .tag.phd { background: #0d2a3d; color: #b3e5fc; }
  html[data-theme='dark'] .tag.publications { background: #4a2e00; color: #ffcc80; }
  html[data-theme='dark'] .tag.cybersecurity { background: #4a1a2e; color: #f8bbd9; }
  html[data-theme='dark'] .tag.azure { background: #0d1b3d; color: #90caf9; }
  html[data-theme='dark'] .tag.automotive { background: #2e1a4a; color: #ce93d8; }
  html[data-theme='dark'] .tag.eu-project { background: #1a3d1a; color: #a5d6a7; }
  html[data-theme='dark'] .tag.business { background: #4a3300; color: #ffcc02; }
  html[data-theme='dark'] .tag.operations { background: #2e2e2e; color: #bdbdbd; }
  html[data-theme='dark'] .tag.computer-vision { background: #003d4a; color: #4dd0e1; }
  html[data-theme='dark'] .tag.medical { background: #1a4a1a; color: #81c784; }

  .experience-grid {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
    gap: 1.5rem;
  }

  .experience-card.small {
    padding: 1.5rem;
  }

  .experience-card.small h3 {
    margin: 0 0 0.25rem 0;
    color: #2c3e50;
    font-size: 1.2rem;
    font-weight: 600;
  }

  html[data-theme='dark'] .experience-card.small h3 {
    color: var(--global-text-color);
  }

  .experience-card.small h4 {
    margin: 0 0 0.5rem 0;
    color: #667eea;
    font-size: 1rem;
    font-weight: 500;
  }

  html[data-theme='dark'] .experience-card.small h4 {
    color: var(--global-theme-color);
  }

  .experience-card.small p {
    color: #495057;
    line-height: 1.5;
    margin: 0.5rem 0;
  }

  html[data-theme='dark'] .experience-card.small p {
    color: var(--global-text-color);
  }

  @media (max-width: 768px) {
    .experience-container {
      padding: 1rem;
    }

    .experience-timeline {
      padding-left: 1.5rem;
    }

    .experience-timeline::before {
      left: 10px;
    }

    .experience-dot {
      left: -22px;
    }

    .experience-card {
      padding: 1.5rem;
    }

    .experience-header {
      flex-direction: column;
      align-items: stretch;
    }

    .role-info h3 {
      font-size: 1.1rem;
    }

    .experience-grid {
      grid-template-columns: 1fr;
    }
  }

  /* Academic Experience Styles */
  .academic-content {
    display: flex;
    flex-direction: column;
    gap: 2rem;
  }

  .academic-item {
    background: var(--global-card-bg-color);
    padding: 1.5rem;
    border-radius: 8px;
    border-left: 4px solid var(--global-theme-color);
    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
    transition: all 0.3s ease;
  }

  html[data-theme='dark'] .academic-item {
    box-shadow: 0 2px 8px rgba(255, 255, 255, 0.05);
  }

  .academic-item:hover {
    transform: translateY(-2px);
    box-shadow: 0 4px 16px rgba(0, 0, 0, 0.15);
  }

  html[data-theme='dark'] .academic-item:hover {
    box-shadow: 0 4px 16px rgba(255, 255, 255, 0.1);
  }

  .academic-item h3 {
    color: var(--global-theme-color);
    margin: 0 0 0.5rem 0;
    font-size: 1.2rem;
    font-weight: 600;
  }

  .academic-item h4 {
    color: var(--global-text-color-light);
    margin: 0 0 0.75rem 0;
    font-size: 1rem;
    font-weight: 500;
  }

  html[data-theme='dark'] .academic-item h4 {
    color: var(--global-text-color);
  }

  .academic-item p {
    color: var(--global-text-color);
    margin: 0 0 0.75rem 0;
    line-height: 1.6;
  }

  .academic-list {
    margin: 0.5rem 0 1rem 1.5rem;
    padding: 0;
  }

  .academic-list li {
    color: var(--global-text-color);
    margin-bottom: 0.5rem;
    line-height: 1.5;
  }

  .institute-link,
  .advisor-link {
    color: var(--global-theme-color);
    text-decoration: none;
    font-weight: 500;
    transition: color 0.3s ease;
  }

  .institute-link:hover,
  .advisor-link:hover {
    color: var(--global-hover-color, #1a73e8);
    text-decoration: underline;
  }

  /* Conference Talks Styles */
  .talk-item {
    background: var(--global-card-bg-color);
    padding: 1.5rem;
    border-radius: 8px;
    border-left: 4px solid #e74c3c;
    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
    margin-bottom: 1.5rem;
    transition: all 0.3s ease;
  }

  html[data-theme='dark'] .talk-item {
    box-shadow: 0 2px 8px rgba(255, 255, 255, 0.05);
  }

  .talk-item:hover {
    transform: translateY(-2px);
    box-shadow: 0 4px 16px rgba(0, 0, 0, 0.15);
  }

  html[data-theme='dark'] .talk-item:hover {
    box-shadow: 0 4px 16px rgba(255, 255, 255, 0.1);
  }

  .talk-title {
    color: #e74c3c;
    margin: 0 0 0.5rem 0;
    font-size: 1.1rem;
    font-weight: 600;
    line-height: 1.4;
  }

  .talk-authors {
    color: var(--global-text-color-light);
    margin: 0 0 0.5rem 0;
    font-style: italic;
    font-size: 0.95rem;
  }

  html[data-theme='dark'] .talk-authors {
    color: var(--global-text-color);
  }

  .talk-venue {
    color: var(--global-text-color);
    margin: 0 0 1rem 0;
    font-weight: 500;
  }

  /* Teaching Styles */
  .teaching-item {
    background: var(--global-card-bg-color);
    padding: 1.5rem;
    border-radius: 8px;
    border-left: 4px solid #27ae60;
    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
    margin-bottom: 1.5rem;
    transition: all 0.3s ease;
  }

  html[data-theme='dark'] .teaching-item {
    box-shadow: 0 2px 8px rgba(255, 255, 255, 0.05);
  }

  .teaching-item:hover {
    transform: translateY(-2px);
    box-shadow: 0 4px 16px rgba(0, 0, 0, 0.15);
  }

  html[data-theme='dark'] .teaching-item:hover {
    box-shadow: 0 4px 16px rgba(255, 255, 255, 0.1);
  }

  .teaching-item h3 {
    color: #27ae60;
    margin: 0 0 0.5rem 0;
    font-size: 1.2rem;
    font-weight: 600;
  }

  .teaching-item h4 {
    color: var(--global-text-color-light);
    margin: 0 0 0.25rem 0;
    font-size: 1rem;
    font-weight: 500;
  }

  html[data-theme='dark'] .teaching-item h4 {
    color: var(--global-text-color);
  }

  .teaching-period {
    color: var(--global-theme-color);
    margin: 0 0 0.75rem 0;
    font-size: 0.9rem;
    font-weight: 500;
  }

  .course-link {
    color: #27ae60;
    text-decoration: none;
    font-weight: 500;
    transition: color 0.3s ease;
  }

  .course-link:hover {
    color: #219a52;
    text-decoration: underline;
  }

  /* Workshop Styles */
  .workshop-item {
    background: var(--global-card-bg-color);
    padding: 1.5rem;
    border-radius: 8px;
    border-left: 4px solid #9b59b6;
    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
    margin-bottom: 1.5rem;
    transition: all 0.3s ease;
  }

  html[data-theme='dark'] .workshop-item {
    box-shadow: 0 2px 8px rgba(255, 255, 255, 0.05);
  }

  .workshop-item:hover {
    transform: translateY(-2px);
    box-shadow: 0 4px 16px rgba(0, 0, 0, 0.15);
  }

  html[data-theme='dark'] .workshop-item:hover {
    box-shadow: 0 4px 16px rgba(255, 255, 255, 0.1);
  }

  .workshop-item h3 {
    margin: 0 0 0.5rem 0;
    font-size: 1.2rem;
    font-weight: 600;
  }

  .workshop-link {
    color: #9b59b6;
    text-decoration: none;
    font-weight: 600;
    transition: color 0.3s ease;
  }

  .workshop-link:hover {
    color: #8e44ad;
    text-decoration: underline;
  }

  .workshop-organizer {
    color: var(--global-text-color-light);
    margin: 0 0 0.25rem 0;
    font-weight: 500;
  }

  html[data-theme='dark'] .workshop-organizer {
    color: var(--global-text-color);
  }

  .workshop-period {
    color: var(--global-theme-color);
    margin: 0 0 0.75rem 0;
    font-size: 0.9rem;
    font-weight: 500;
  }

  /* Additional Tag Styles for New Categories */
  .tag.peer-review { background: linear-gradient(135deg, #e67e22, #d35400); }
  .tag.international { background: linear-gradient(135deg, #3498db, #2980b9); }
  .tag.mit { background: linear-gradient(135deg, #a53860, #8e2a4f); }
  .tag.innovation { background: linear-gradient(135deg, #f39c12, #e67e22); }
  .tag.iisc { background: linear-gradient(135deg, #8e44ad, #7d3c98); }
  .tag.3d-vision { background: linear-gradient(135deg, #16a085, #138d75); }
  .tag.iit { background: linear-gradient(135deg, #c0392b, #a93226); }
  .tag.signal-processing { background: linear-gradient(135deg, #2ecc71, #27ae60); }
  .tag.paleontology { background: linear-gradient(135deg, #795548, #6d4c41); }
  .tag.conference { background: linear-gradient(135deg, #ff5722, #e64a19); }
  .tag.deep-learning { background: linear-gradient(135deg, #673ab7, #5e35b1); }
  .tag.botany { background: linear-gradient(135deg, #4caf50, #43a047); }
  .tag.virtual { background: linear-gradient(135deg, #607d8b, #546e7a); }
  .tag.automation { background: linear-gradient(135deg, #ff9800, #f57c00); }
  .tag.visual-reasoning { background: linear-gradient(135deg, #e91e63, #c2185b); }
  .tag.brown { background: linear-gradient(135deg, #795548, #5d4037); }
  .tag.computational { background: linear-gradient(135deg, #009688, #00796b); }
  .tag.teaching { background: linear-gradient(135deg, #4caf50, #388e3c); }
  .tag.supervision { background: linear-gradient(135deg, #2196f3, #1976d2); }
  .tag.france { background: linear-gradient(135deg, #3f51b5, #303f9f); }
  .tag.electronics { background: linear-gradient(135deg, #ff5722, #d84315); }
  .tag.lab { background: linear-gradient(135deg, #607d8b, #455a64); }
  .tag.reinforcement-learning { background: linear-gradient(135deg, #9c27b0, #7b1fa2); }
  .tag.aniti { background: linear-gradient(135deg, #ff9800, #ef6c00); }
  .tag.neuroscience { background: linear-gradient(135deg, #e91e63, #ad1457); }
  .tag.coursera { background: linear-gradient(135deg, #2196f3, #0d47a1); }
  .tag.psychiatry { background: linear-gradient(135deg, #795548, #4e342e); }
  .tag.zurich { background: linear-gradient(135deg, #f44336, #c62828); }

  /* Responsive Design for New Sections */
  @media (max-width: 768px) {
    .academic-item,
    .talk-item,
    .teaching-item,
    .workshop-item {
      padding: 1rem;
      margin-bottom: 1rem;
    }

    .academic-item h3,
    .teaching-item h3,
    .workshop-item h3 {
      font-size: 1.1rem;
    }

    .talk-title {
      font-size: 1rem;
    }
  }
</style>


  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2025 Mohit  Vaishnav.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    Last updated: October 26, 2025.
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>


</html>
