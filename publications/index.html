<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Mohit  Vaishnav | Publications</title>
<meta name="description" content="Research publications and academic contributions in artificial intelligence, computer vision, and machine learning.  Published in top-tier venues including NeurIPS, ICLR, Neural Computation, CVPR workshops, and specialized conferences.  Research areas include visual reasoning, attention mechanisms, transformer architectures, cognitive AI, medical imaging,  and paleobotany applications. Google Scholar verified publications with international collaborations.">
<meta name="keywords" content="AI Publications, Research Papers, NeurIPS, ICLR, CVPR, Neural Computation, Computer Vision Research,  Deep Learning Papers, Visual Reasoning, Attention Mechanisms, Transformer Architecture, Cognitive AI,  Machine Learning Research, Academic Publications, Google Scholar, ResearchGate, AI Conference Papers,  Medical Imaging AI, Paleobotany AI, Abstract Reasoning, Mohit Vaishnav, AI Entrepreneur, CEO Kimova AI, Cybersecurity AI, ISO 27001 Compliance, Computer Vision, Deep Learning,  Visual Reasoning, PhD Artificial Intelligence, Brown University, ANITI, Cognitive AI, Transformer Architecture,  Machine Learning Expert, AI Research, Enterprise AI, Google Cloud Startup, NeurIPS, ICLR, Kaggle Competition Winner,  DFKI Researcher, Data Scientist, AI CEO, Technology Leadership, Innovation
">
<meta name="author" content="Dr. Mohit Vaishnav Vaishnav">
<meta name="robots" content="index, follow">

<!-- Additional SEO Meta Tags -->
<meta name="language" content="en">
<meta name="revisit-after" content="7 days">
<meta name="classification" content="Artificial Intelligence, Machine Learning, Computer Vision, Research">
<link rel="canonical" href="https://vaishnavmohit.github.io/publications/">

<!-- Twitter Card Meta Tags -->
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@mohitvaishnav">
<meta name="twitter:creator" content="@mohitvaishnav">
<meta name="twitter:title" content="Publications | Mohit Vaishnav">
<meta name="twitter:description" content="Research publications and academic contributions in artificial intelligence, computer vision, and machine learning.  Published in top-tier venues including NeurIPS, ICLR, Neural Computation, CVPR workshops, and specialized conferences.  Research areas include visual reasoning, attention mechanisms, transformer architectures, cognitive AI, medical imaging,  and paleobotany applications. Google Scholar verified publications with international collaborations.">
<meta name="twitter:image" content="https://vaishnavmohit.github.io/assets/img/Kimova_Icon_small.png">
<!-- Schema.org Person JSON-LD for Google Knowledge Panel -->
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Person",
  "name": "Mohit Vaishnav",
  "image": "https://vaishnavmohit.github.io/assets/img/mohit_vaishnav.jpg",
  "jobTitle": "AI Entrepreneur & Researcher",
  "worksFor": {
    "@type": "Organization",
    "name": "Kimova AI",
    "url": "https://kimova.ai"
  },
  "alumniOf": [
    {
      "@type": "CollegeOrUniversity",
      "name": "Taltech University",
      "url": "https://taltech.ee/en"
    },
    {
      "@type": "CollegeOrUniversity",
      "name": "Brown University",
      "url": "https://brown.edu"
    },
    {
      "@type": "CollegeOrUniversity",
      "name": "ANITI",
      "url": "https://aniti.univ-toulouse.fr/"
    },
    {
      "@type": "CollegeOrUniversity",
      "name": "University of Toulouse III - Paul Sabatier",
      "url": "https://www.univ-tlse3.fr/home"
    },
    {
      "@type": "CollegeOrUniversity",
      "name": "Université Bourgogne Franche-Comté",
      "url": "https://www.u-bourgogne.fr/"
    },
    {
      "@type": "CollegeOrUniversity",
      "name": "Heriot-Watt University",
      "url": "https://www.hw.ac.uk/"
    },
    {
      "@type": "CollegeOrUniversity",
      "name": "LNM Institute of Information Technology",
      "url": "https://www.lnmiit.ac.in/"
    }
  ],
  "description": "Co-founder at Kimova AI (AI for ISO Cybersecurity Compliance). PhD in Cognitive AI & Visual Reasoning (Brown/ANITI). Bridging research with real-world impact.",
  "sameAs": [
  "https://github.com/vaishnavmohit",
  "https://twitter.com/mohitvaishnav",
  "https://www.linkedin.com/in/mohit-vaishnav",
  "https://scholar.google.com/citations?user=jGOzdbgAAAAJ&hl",
  "https://www.researchgate.net/profile/Mohit-Vaishnav-2",
  "https://serre-lab.clps.brown.edu/person/mohit-vaishnav/",
  "https://orcid.org/0000-0002-9795-493X",
  "https://www.etis.ee/CV/mohit-vaishnav/eng/",
  "https://openreview.net/profile?id=~Mohit_Vaishnav1",
  "https://cv.hal.science/mohitvaishnav",
  "https://dblp.org/pid/82/9414.html"
  ],
  "url": "https://vaishnavmohit.github.io/"
}
</script>

<!-- Open Graph Meta Tags -->

<meta property="og:site_name" content="Mohit Vaishnav - AI Entrepreneur & Research Leader | CEO at Kimova AI | PhD in Cognitive AI" />
<meta property="og:type" content="website" />
<meta property="og:title" content="Publications | Mohit Vaishnav" />
<meta property="og:url" content="https://vaishnavmohit.github.io/publications/" />
<meta property="og:description" content="Research publications and academic contributions in artificial intelligence, computer vision, and machine learning.  Published in top-tier venues including NeurIPS, ICLR, Neural Computation, CVPR workshops, and specialized conferences.  Research areas include visual reasoning, attention mechanisms, transformer architectures, cognitive AI, medical imaging,  and paleobotany applications. Google Scholar verified publications with international collaborations." />
<meta property="og:image" content="https://vaishnavmohit.github.io/assets/img/Kimova_Icon_small.png" />
<meta property="og:image:alt" content="Mohit Vaishnav" />
<meta property="og:locale" content="en_US" />




<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<!-- Theming-->

  <script src="/assets/js/theme.js"></script>



  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-CJ1FLXGREG"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-CJ1FLXGREG');
  </script>
<!-- Schema.org ScholarlyArticle JSON-LD for Publications -->

<script type="application/ld+json">
[  {    "@context": "https://schema.org",    "@type": "ScholarlyArticle",    "name": "PhD Thesis: Exploring the role of (self-)attention in cognitive and computer vision architecture",    "author": {      "@type": "Person",      "name": "Mohit Vaishnav"    },    "datePublished": "2023",    "url": "https://arxiv.org/abs/2306.14650",    "description": "We investigate the role of attention and memory in complex reasoning tasks. We analyze Transformer-based self-attention as a model and extend it with memory. By studying a synthetic visual reasoning test, we refine the taxonomy of reasoning tasks. Incorporating self-attention with ResNet50, we enhance feature maps using feature-based and spatial attention, achieving efficient solving of challenging visual reasoning tasks. Our findings contribute to understanding the attentional needs of SVRT tasks. Additionally, we propose GAMR, a cognitive architecture combining attention and memory, inspired by active vision theory. GAMR outperforms other architectures in sample efficiency, robustness, and compositionality, and shows zero-shot generalization on new reasoning tasks."  },  {    "@context": "https://schema.org",    "@type": "ScholarlyArticle",    "name": "GAMR: Guided Attention Model for (visual) Reasoning",    "author": {      "@type": "Person",      "name": "Mohit Vaishnav"    },    "datePublished": "2023",    "url": "https://openreview.net/forum?id=iLMgk2IGNyv",    "description": "Humans continue to outperform modern AI systems in their ability to flexibly parse and understand complex visual scenes. Here, we present a novel module for visual reasoning, the Guided Attention Model for (visual) Reasoning (GAMR), which instantiates an active vision theory -- positing that the brain solves complex visual reasoning problems dynamically -- via sequences of attention shifts to select and route task-relevant visual information into memory. Experiments on an array of visual reasoning tasks and datasets demonstrate GAMR's ability to learn visual routines in a robust and sample-efficient manner. In addition, GAMR is shown to be capable of zero-shot generalization on completely novel reasoning tasks. Overall, our work provides computational support for cognitive theories that postulate the need for a critical interplay between attention and memory to dynamically maintain and manipulate task-relevant visual information to solve complex visual reasoning tasks."  },  {    "@context": "https://schema.org",    "@type": "ScholarlyArticle",    "name": "Understanding the Computational Demands Underlying Visual Reasoning",    "author": {      "@type": "Person",      "name": "Mohit Vaishnav"    },    "datePublished": "2022",    "url": "https://doi.org/10.1162/neco_a_01485",    "description": "Visual understanding requires comprehending complex visual relations between objects within a scene. Here, we seek to characterize the computational demands for abstract visual reasoning. We do this by systematically assessing the ability of modern deep convolutional neural networks (CNNs) to learn to solve the synthetic visual reasoning test (SVRT) challenge, a collection of 23 visual reasoning problems. Our analysis reveals a novel taxonomy of visual reasoning tasks, which can be primarily explained by both the type of relations (same-different versus spatial-relation judgments) and the number of relations used to compose the underlying rules. Prior cognitive neuroscience work suggests that attention plays a key role in humans' visual reasoning ability. To test this hypothesis, we extended the CNNs with spatial and feature-based attention mechanisms. In a second series of experiments, we evaluated the ability of these attention networks to learn to solve the SVRT challenge and found the resulting architectures to be much more efficient at solving the hardest of these visual reasoning tasks. Most important, the corresponding improvements on individual tasks partially explained our novel taxonomy. Overall, this work provides a granular computational account of visual reasoning and yields testable neuroscience predictions regarding the differential need for feature-based versus spatial attention depending on the type of visual reasoning problem."  },  {    "@context": "https://schema.org",    "@type": "ScholarlyArticle",    "name": "Conviformers: Convolutionally guided Vision Transformer",    "author": {      "@type": "Person",      "name": "Mohit Vaishnav"    },    "datePublished": "2022",    "url": "https://arxiv.org/pdf/2208.08900",    "description": "Vision transformers are nowadays the de-facto preference for image classification tasks. There are two broad categories of classification tasks, fine-grained and coarse-grained. In fine-grained classification, the necessity is to discover subtle differences due to the high level of similarity between sub-classes. Such distinctions are often lost as we downscale the image to save the memory and computational cost associated with vision transformers (ViT). In this work, we present an in-depth analysis and describe the critical components for developing a system for the fine-grained categorization of plants from herbarium sheets. Our extensive experimental analysis indicated the need for a better augmentation technique and the ability of modern-day neural networks to handle higher dimensional images. We also introduce a convolutional transformer architecture called Conviformer which, unlike the popular Vision Transformer (ConViT), can handle higher resolution images without exploding memory and computational cost. We also introduce a novel, improved pre-processing technique called PreSizer to resize images better while preserving their original aspect ratios, which proved essential for classifying natural plants. With our simple yet effective approach, we achieved SoTA on Herbarium 202x and iNaturalist 2019 dataset."  },  {    "@context": "https://schema.org",    "@type": "ScholarlyArticle",    "name": "A Benchmark for Compositional Visual Reasoning",    "author": {      "@type": "Person",      "name": "Mohit Vaishnav"    },    "datePublished": "2022",    "url": "https://arxiv.org/pdf/2206.05379",    "description": "A fundamental component of human vision is our ability to parse complex visual scenes and judge the relations between their constituent objects. AI benchmarks for visual reasoning have driven rapid progress in recent years with state-of-the-art systems now reaching human accuracy on some of these benchmarks. Yet, a major gap remains in terms of the sample efficiency with which humans and AI systems learn new visual reasoning tasks. Humans' remarkable efficiency at learning has been at least partially attributed to their ability to harness compositionality -- such that they can efficiently take advantage of previously gained knowledge when learning new tasks. Here, we introduce a novel visual reasoning benchmark, Compositional Visual Relations (CVR), to drive progress towards the development of more data-efficient learning algorithms. We take inspiration from fluidic intelligence and non-verbal reasoning tests and describe a novel method for creating compositions of abstract rules and associated image datasets at scale. Our proposed benchmark includes measures of sample efficiency, generalization and transfer across task rules, as well as the ability to leverage compositionality. We systematically evaluate modern neural architectures and find that, surprisingly, convolutional architectures surpass transformer-based architectures across all performance measures in most data regimes. However, all computational models are a lot less data efficient compared to humans even after learning informative visual representations using self-supervision. Overall, we hope that our challenge will spur interest in the development of neural architectures that can learn to harness compositionality toward more efficient learning."  }]
</script>


    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="https://vaishnavmohit.github.io/">
       <span class="font-weight-bold">Mohit</span>   Vaishnav
      </a>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          
          <!-- Other pages -->
          
          
          
            
          
            
              <li class="nav-item ">
                <a class="nav-link" href="/experience_backup/">
                  
                  
                </a>
              </li>
            
          
            
          
            
          
            
          
            
          
            
          
            
              <li class="nav-item ">
                <a class="nav-link" href="/education/">
                  Education
                  
                </a>
              </li>
            
          
            
              <li class="nav-item ">
                <a class="nav-link" href="/experience/">
                  Experience
                  
                </a>
              </li>
            
          
            
          
            
          
            
          
            
              <li class="nav-item ">
                <a class="nav-link" href="/awards/">
                  Recognition
                  
                </a>
              </li>
            
          
          <!-- Research Dropdown -->
          <li class="nav-item dropdown active">
  <a class="nav-link" href="#" id="navbarDropdownResearch">
    Research
  </a>
            <div class="dropdown-menu" aria-labelledby="navbarDropdownResearch">
              <a class="dropdown-item" href="/publications/">Publications</a>
              <a class="dropdown-item" href="/phd-thesis/">PhD Thesis</a>
            </div>
          </li>
    <!-- CV -->
          <li class="nav-item ">
      <a class="nav-link" target="_blank" href="/assets/pdf/Mohit_Vaishnav_cv.pdf">CV</a>
          </li>
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">


  <article>
    <!-- JSON-LD Structured Data for Publications -->
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Person",
  "name": "Mohit Vaishnav",
  "identifier": {
    "@type": "PropertyValue",
    "propertyID": "Google Scholar",
    "value": "jGOzdbgAAAAJ"
  },
  "sameAs": [
    "https://scholar.google.com/citations?user=jGOzdbgAAAAJ",
    "https://www.researchgate.net/profile/Mohit-Vaishnav-2"
  ],
  "author": {
    "@type": "Person",
    "name": "Mohit Vaishnav"
  },
  "keywords": [
    "Artificial Intelligence",
    "Machine Learning", 
    "Computer Vision",
    "Visual Reasoning",
    "Attention Mechanisms",
    "Transformer Architecture",
    "Cognitive AI",
    "Deep Learning"
  ],
  "description": "Research publications in artificial intelligence and computer vision, published in top-tier conferences and journals including NeurIPS, ICLR, and Neural Computation"
}
</script>

<div class="publications-container">
  <div class="publications-header">
    <p class="publications-subtitle">Explore my research work organized by year</p>
  </div>

  <div class="publications-timeline">
    
      <div class="year-section" data-year="2023">
        <div class="year-header">
          <h2 class="year-title">2023</h2>
          <div class="year-line"></div>
        </div>
        <div class="publications-content">
          <ol class="bibliography"><li><div class="row">
  <div class="col-sm-3">
    
      <img class="img-fluid" src="/assets/pubimg/phdthesis.png" />
    
  </div>

  <div id="vaishnav2023phd" class="col-sm-8">
    
      <div class="title">PhD Thesis: Exploring the role of (self-)attention in cognitive and computer vision architecture</div>
      <div class="author">
        
          
          
            
              <em>Vaishnav, Mohit</em>
            
          
        
      </div>

      <div class="periodical">
        
          <em>arXiv, cs.AI</em>
        
        
          2023
        
        
      </div>
    

    <div class="links">
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
      
        <a href="https://arxiv.org/abs/2306.14650" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
      
      
      
      
      
      
      
      
    </div>

    <!-- Hidden abstract block -->
    
      <div class="abstract hidden">
        <p>We investigate the role of attention and memory in complex reasoning tasks. We analyze Transformer-based self-attention as a model and extend it with memory. By studying a synthetic visual reasoning test, we refine the taxonomy of reasoning tasks. Incorporating self-attention with ResNet50, we enhance feature maps using feature-based and spatial attention, achieving efficient solving of challenging visual reasoning tasks. Our findings contribute to understanding the attentional needs of SVRT tasks. Additionally, we propose GAMR, a cognitive architecture combining attention and memory, inspired by active vision theory. GAMR outperforms other architectures in sample efficiency, robustness, and compositionality, and shows zero-shot generalization on new reasoning tasks.</p>
      </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
    
      <img class="img-fluid" src="/assets/pubimg/GAMR.png" />
    
  </div>

  <div id="vaishnav2022mareo" class="col-sm-8">
    
      <div class="title">GAMR: Guided Attention Model for (visual) Reasoning</div>
      <div class="author">
        
          
          
            
              <em>Vaishnav, Mohit</em>
            
          
        
          
          
            
              and <em>Serre, Thomas</em>
            
          
        
      </div>

      <div class="periodical">
        
          <em>International Conference on Learning Representations ICLR</em>
        
        
          2023
        
        
      </div>
    

    <div class="links">
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
      
        <a href="https://openreview.net/forum?id=iLMgk2IGNyv" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
      
      
      
      
      
      
      
      
    </div>

    <!-- Hidden abstract block -->
    
      <div class="abstract hidden">
        <p>Humans continue to outperform modern AI systems in their ability to flexibly parse and understand complex visual scenes. Here, we present a novel module for visual reasoning, the Guided Attention Model for (visual) Reasoning (GAMR), which instantiates an active vision theory – positing that the brain solves complex visual reasoning problems dynamically – via sequences of attention shifts to select and route task-relevant visual information into memory. Experiments on an array of visual reasoning tasks and datasets demonstrate GAMR’s ability to learn visual routines in a robust and sample-efficient manner. In addition, GAMR is shown to be capable of zero-shot generalization on completely novel reasoning tasks. Overall, our work provides computational support for cognitive theories that postulate the need for a critical interplay between attention and memory to dynamically maintain and manipulate task-relevant visual information to solve complex visual reasoning tasks.</p>
      </div>
    
  </div>
</div>
</li></ol>
        </div>
      </div>
    
      <div class="year-section" data-year="2022">
        <div class="year-header">
          <h2 class="year-title">2022</h2>
          <div class="year-line"></div>
        </div>
        <div class="publications-content">
          <ol class="bibliography"><li><div class="row">
  <div class="col-sm-3">
    
      <img class="img-fluid" src="/assets/pubimg/nc_resnet.png" />
    
  </div>

  <div id="vaishnav2021understanding" class="col-sm-8">
    
      <div class="title">Understanding the Computational Demands Underlying Visual Reasoning</div>
      <div class="author">
        
          
          
            
              <em>Vaishnav, Mohit</em>
            
          
        
          
          
            
              <em>Cadene, Remi</em>,
            
          
        
          
          
            
              <em>Alamia, Andrea</em>,
            
          
        
          
          
            
              <em>Linsley, Drew</em>,
            
          
        
          
          
            
              <em>VanRullen, Rufin</em>,
            
          
        
          
          
            
              and <em>Serre, Thomas</em>
            
          
        
      </div>

      <div class="periodical">
        
          <em>Special Collection CogNet of Neural Computation</em>
        
        
          2022
        
        
      </div>
    

    <div class="links">
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
      
        <a href="https://direct.mit.edu/neco/article-pdf/doi/10.1162/neco_a_01485/1993419/neco_a_01485.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
      
      
      
      
      
      
      
      
    </div>

    <!-- Hidden abstract block -->
    
      <div class="abstract hidden">
        <p>Visual understanding requires comprehending complex visual relations between objects within a scene. Here, we seek to characterize the computational demands for abstract visual reasoning. We do this by systematically assessing the ability of modern deep convolutional neural networks (CNNs) to learn to solve the synthetic visual reasoning test (SVRT) challenge, a collection of 23 visual reasoning problems. Our analysis reveals a novel taxonomy of visual reasoning tasks, which can be primarily explained by both the type of relations (same-different versus spatial-relation judgments) and the number of relations used to compose the underlying rules. Prior cognitive neuroscience work suggests that attention plays a key role in humans’ visual reasoning ability. To test this hypothesis, we extended the CNNs with spatial and feature-based attention mechanisms. In a second series of experiments, we evaluated the ability of these attention networks to learn to solve the SVRT challenge and found the resulting architectures to be much more efficient at solving the hardest of these visual reasoning tasks. Most important, the corresponding improvements on individual tasks partially explained our novel taxonomy. Overall, this work provides a granular computational account of visual reasoning and yields testable neuroscience predictions regarding the differential need for feature-based versus spatial attention depending on the type of visual reasoning problem.</p>
      </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
    
      <img class="img-fluid" src="/assets/pubimg/conviformer.jpg" />
    
  </div>

  <div id="vaishnav2022conviformers" class="col-sm-8">
    
      <div class="title">Conviformers: Convolutionally guided Vision Transformer</div>
      <div class="author">
        
          
          
            
              <em>Vaishnav, Mohit</em>
            
          
        
          
          
            
              <em>Fel, Thomas</em>,
            
          
        
          
          
            
              <em>Rodrıguez, Ivan Felipe</em>,
            
          
        
          
          
            
              and <em>Serre, Thomas</em>
            
          
        
      </div>

      <div class="periodical">
        
          <em>arXiv preprint arXiv:2208.08900</em>
        
        
          2022
        
        
      </div>
    

    <div class="links">
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
      
        <a href="https://arxiv.org/pdf/2208.08900" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
      
      
      
      
      
      
      
      
    </div>

    <!-- Hidden abstract block -->
    
      <div class="abstract hidden">
        <p>Vision transformers are nowadays the de-facto preference for image classification tasks. There are two broad categories of classification tasks, fine-grained and coarse-grained. In fine-grained classification, the necessity is to discover subtle differences due to the high level of similarity between sub-classes. Such distinctions are often lost as we downscale the image to save the memory and computational cost associated with vision transformers (ViT). In this work, we present an in-depth analysis and describe the critical components for developing a system for the fine-grained categorization of plants from herbarium sheets. Our extensive experimental analysis indicated the need for a better augmentation technique and the ability of modern-day neural networks to handle higher dimensional images. We also introduce a convolutional transformer architecture called Conviformer which, unlike the popular Vision Transformer (ConViT), can handle higher resolution images without exploding memory and computational cost. We also introduce a novel, improved pre-processing technique called PreSizer to resize images better while preserving their original aspect ratios, which proved essential for classifying natural plants. With our simple yet effective approach, we achieved SoTA on Herbarium 202x and iNaturalist 2019 dataset.</p>
      </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
    
      <img class="img-fluid" src="/assets/pubimg/CVR.png" />
    
  </div>

  <div id="zerroug2022benchmark" class="col-sm-8">
    
      <div class="title">A Benchmark for Compositional Visual Reasoning</div>
      <div class="author">
        
          
          
            
              <em>Zerroug, Aimen</em>
            
          
        
          
          
            
              <em>Vaishnav, Mohit</em>,
            
          
        
          
          
            
              <em>Colin, Julien</em>,
            
          
        
          
          
            
              <em>Musslick, Sebastian</em>,
            
          
        
          
          
            
              and <em>Serre, Thomas</em>
            
          
        
      </div>

      <div class="periodical">
        
          <em>In Proceedings of the Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks</em>
        
        
          2022
        
        
      </div>
    

    <div class="links">
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
      
        <a href="https://arxiv.org/pdf/2206.05379" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
      
      
      
      
      
      
      
      
    </div>

    <!-- Hidden abstract block -->
    
      <div class="abstract hidden">
        <p>A fundamental component of human vision is our ability to parse complex visual scenes and judge the relations between their constituent objects. AI benchmarks for visual reasoning have driven rapid progress in recent years with state-of-the-art systems now reaching human accuracy on some of these benchmarks. Yet, a major gap remains in terms of the sample efficiency with which humans and AI systems learn new visual reasoning tasks. Humans’ remarkable efficiency at learning has been at least partially attributed to their ability to harness compositionality – such that they can efficiently take advantage of previously gained knowledge when learning new tasks. Here, we introduce a novel visual reasoning benchmark, Compositional Visual Relations (CVR), to drive progress towards the development of more data-efficient learning algorithms. We take inspiration from fluidic intelligence and non-verbal reasoning tests and describe a novel method for creating compositions of abstract rules and associated image datasets at scale. Our proposed benchmark includes measures of sample efficiency, generalization and transfer across task rules, as well as the ability to leverage compositionality. We systematically evaluate modern neural architectures and find that, surprisingly, convolutional architectures surpass transformer-based architectures across all performance measures in most data regimes. However, all computational models are a lot less data efficient compared to humans even after learning informative visual representations using self-supervision. Overall, we hope that our challenge will spur interest in the development of neural architectures that can learn to harness compositionality toward more efficient learning.</p>
      </div>
    
  </div>
</div>
</li></ol>
        </div>
      </div>
    
      <div class="year-section" data-year="2014">
        <div class="year-header">
          <h2 class="year-title">2014</h2>
          <div class="year-line"></div>
        </div>
        <div class="publications-content">
          <ol class="bibliography"><li><div class="row">
  <div class="col-sm-3">
    
      <img class="img-fluid" src="/assets/pubimg/residuecoding.png" />
    
  </div>

  <div id="DBLP:conf/dcc/VaishnavTT14" class="col-sm-8">
    
      <div class="title">Residue Coding Technique for Video Compression</div>
      <div class="author">
        
          
          
            
              <em>Vaishnav, Mohit</em>
            
          
        
          
          
            
              <em>Tewani, Binny</em>,
            
          
        
          
          
            
              and <em>Tiwari, Anil Kumar</em>
            
          
        
      </div>

      <div class="periodical">
        
          <em>In Data Compression Conference, DCC, Snowbird, UT, USA, 26-28
               March</em>
        
        
          2014
        
        
      </div>
    

    <div class="links">
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
      
        <a href="https://ieeexplore.ieee.org/document/6824481/" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
      
      
      
      
      
      
      
      
    </div>

    <!-- Hidden abstract block -->
    
      <div class="abstract hidden">
        <p>All the methods available in literature for lossless coding comprises of two steps, one is the prediction scheme that results into prediction error and the second one consists of entropy coding of prediction error samples. In this work we introduce a scheme which adds a step prior to entropy coding technique due to which, the achieved gain in terms of compression is high. The additional step works on the predicted error frames obtained after applying any prediction scheme. The error frames so obtained are remapped into another domain of 8 bits per error sample. While remapping, two files are generated which needs to be compressed, One is the remapped error file itself while the second file consists of overhead information needed to get back the original error samples. The second file consists of binary numbers. The remapped file is compressed by CALIC encoder while the other is kept as it is, causing an addition of one bit per pixel overhead to the compressed file. Results show that instead of going in a traditional manner if we follow the proposed path, there will be a gain of about 0.21 bpp on an average. Additionally the comparison has also been made against other prominent codecs like JPEG and CALIC.</p>
      </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
    
      <img class="img-fluid" src="/assets/pubimg/binclassification.png" />
    
  </div>

  <div id="DBLP:conf/dcc/VaishnavT14" class="col-sm-8">
    
      <div class="title">Bin Classification Using Temporal Gradient Estimation for Lossless
               Video Coding</div>
      <div class="author">
        
          
          
            
              <em>Vaishnav, Mohit</em>
            
          
        
          
          
            
              and <em>Tiwari, Anil Kumar</em>
            
          
        
      </div>

      <div class="periodical">
        
          <em>In Data Compression Conference, DCC, Snowbird, UT, USA, 26-28
               March</em>
        
        
          2014
        
        
      </div>
    

    <div class="links">
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
      
        <a href="https://ieeexplore.ieee.org/document/6824482/" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
      
      
      
      
      
      
      
      
    </div>

    <!-- Hidden abstract block -->
    
      <div class="abstract hidden">
        <p>In this paper, a novel method for lossless compression of video is proposed. Almost all the prediction based methods reported in literature are of two pass. In the first pass, motion compensated frame is obtained and in the second, some sophisticated method is used to predict the pixels of the current frame. The proposed method is an efficient replacement for the first method that predicts current pixel using an estimate of deviation from the pixel at same temporal location in the previous frame. In this scheme, causal pixels are divided into bins based on the distance between the current and causal pixels. The novelty of the work is in finding out the fixed coefficients of the bins for a particular type of video sequence. The overall performance of the proposed method is same with much lower computational complexity.</p>
      </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
    
      <img class="img-fluid" src="/assets/pubimg/temporalstationarity.png" />
    
  </div>

  <div id="DBLP:conf/icvgip/VaishnavCT14" class="col-sm-8">
    
      <div class="title">Temporal Stationarity Based Prediction Method For Lossless Video Coding</div>
      <div class="author">
        
          
          
            
              <em>Vaishnav, Mohit</em>
            
          
        
          
          
            
              <em>Chobey, Dinesh Kumar</em>,
            
          
        
          
          
            
              and <em>Tiwari, Anil Kumar</em>
            
          
        
      </div>

      <div class="periodical">
        
          <em>In Proceedings of Indian Conference on Computer Vision, Graphics
               and Image Processing, ICVGIP, Bangalore, India, December 14-18</em>
        
        
          2014
        
        
      </div>
    

    <div class="links">
      
      
      
      
      
      
      
      
      
      
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li></ol>
        </div>
      </div>
    
      <div class="year-section" data-year="2013">
        <div class="year-header">
          <h2 class="year-title">2013</h2>
          <div class="year-line"></div>
        </div>
        <div class="publications-content">
          <ol class="bibliography"><li><div class="row">
  <div class="col-sm-3">
    
  </div>

  <div id="DBLP:conf/dcc/ChobeyVT13" class="col-sm-8">
    
      <div class="title">An Optimal Switched Adaptive Prediction Method for Lossless Video
               Coding</div>
      <div class="author">
        
          
          
            
              <em>Chobey, Dinesh Kumar</em>
            
          
        
          
          
            
              <em>Vaishnav, Mohit</em>,
            
          
        
          
          
            
              and <em>Tiwari, Anil Kumar</em>
            
          
        
      </div>

      <div class="periodical">
        
          <em>In Data Compression Conference, DCC, Snowbird, UT, USA, March
               20-22</em>
        
        
          2013
        
        
      </div>
    

    <div class="links">
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
      
        <a href="https://ieeexplore.ieee.org/document/6543091/" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
      
      
      
      
      
      
      
      
    </div>

    <!-- Hidden abstract block -->
    
      <div class="abstract hidden">
        <p>In this work, we propose a method of lossless video coding which not has only the decoder simple but encoder is also simple, unlike other reported methods which has computationally complex encoder. The computation is mainly due to not using motion compensation method, which is computationally complex process. The coefficient of the predictors are obtained based on an averaging process and then the thus obtained set of switched predictors is used for prediction. The parameters have been obtained after undergoing a statistical process of averaging so that proper relationship can be established between the predicted pixel and their context.</p>
      </div>
    
  </div>
</div>
</li></ol>
        </div>
      </div>
    
      <div class="year-section" data-year="2011">
        <div class="year-header">
          <h2 class="year-title">2011</h2>
          <div class="year-line"></div>
        </div>
        <div class="publications-content">
          <ol class="bibliography"><li><div class="row">
  <div class="col-sm-3">
    
      <img class="img-fluid" src="/assets/pubimg/2011.png" />
    
  </div>

  <div id="DBLP:conf/dcc/VaishnavST11" class="col-sm-8">
    
      <div class="title">A Novel Computationally Efficient Motion Compensation Method Based
               on Pixel by Pixel Prediction</div>
      <div class="author">
        
          
          
            
              <em>Vaishnav, Mohit</em>
            
          
        
          
          
            
              <em>Sharma, Ashwani</em>,
            
          
        
          
          
            
              and <em>Tiwari, Anil Kumar</em>
            
          
        
      </div>

      <div class="periodical">
        
          <em>In Data Compression Conference (DCC), 29-31 March, Snowbird,
               UT, USA</em>
        
        
          2011
        
        
      </div>
    

    <div class="links">
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
      
        <a href="https://ieeexplore.ieee.org/document/5749537" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
      
      
      
      
      
      
      
      
    </div>

    <!-- Hidden abstract block -->
    
      <div class="abstract hidden">
        <p>This paper presents a novel loss less compression method for video. In this work, we propose a novel method for finding motion compensated frame. This is computationally much efficient than other method reported in literature. After finding the motion compensated frame, we propose a new method for efficiently applying LS based predictor on the frames. The predictor structure uses pixels in the current frame and also in the motion compensated frame. Overall performance of the proposed method is significantly better than many competitive methods at significantly reduced computational complexity.</p>
      </div>
    
  </div>
</div>
</li></ol>
        </div>
      </div>
    
  </div>
</div>

<style>
  .publications-container {
    max-width: 1000px;
    margin: 0 auto;
    padding: 2rem;
    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
  }

  .publications-header {
    text-align: center;
    margin-bottom: 3rem;
  }

  .publications-subtitle {
    color: #6c757d;
    font-size: 1.1rem;
    margin: 0;
    font-weight: 400;
  }

  html[data-theme='dark'] .publications-subtitle {
    color: #aaa;
  }

  .publications-timeline {
    position: relative;
  }

  .publications-timeline::before {
    content: '';
    position: absolute;
    left: 50px;
    top: 0;
    bottom: 0;
    width: 2px;
    background: linear-gradient(180deg, #667eea 0%, #764ba2 100%);
    border-radius: 1px;
  }

  html[data-theme='dark'] .publications-timeline::before {
    background: linear-gradient(180deg, var(--global-theme-color) 0%, #764ba2 100%);
  }

  .year-section {
    margin-bottom: 3rem;
    position: relative;
  }

  .year-header {
    display: flex;
    align-items: center;
    margin-bottom: 1.5rem;
    position: relative;
  }

    .year-title {
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    color: white;
    padding: 0.75rem 1.5rem;
    border-radius: 25px;
    margin: 0;
    font-size: 1.5rem;
    font-weight: 600;
    box-shadow: 0 4px 15px rgba(102, 126, 234, 0.3);
    position: relative;
    z-index: 2;
  }

  html[data-theme='dark'] .year-title {
    background: linear-gradient(135deg, var(--global-theme-color) 0%, #764ba2 100%);
  }

  .year-title::before {
    content: '';
    position: absolute;
    left: -25px;
    top: 50%;
    transform: translateY(-50%);
    width: 12px;
    height: 12px;
    background: white;
    border: 3px solid #667eea;
    border-radius: 50%;
    box-shadow: 0 2px 8px rgba(102, 126, 234, 0.3);
  }

  html[data-theme='dark'] .year-title::before {
    background: var(--global-card-bg-color);
    border: 3px solid var(--global-theme-color);
  }

  .year-line {
    flex: 1;
    height: 1px;
    background: linear-gradient(90deg, #667eea 0%, transparent 100%);
    margin-left: 2rem;
  }

  html[data-theme='dark'] .year-line {
    background: linear-gradient(90deg, var(--global-theme-color) 0%, transparent 100%);
  }

  .publications-content {
    margin-left: 100px;
    padding: 1.5rem;
    background: linear-gradient(135deg, #f8f9fa 0%, #ffffff 100%);
    border-radius: 12px;
    border: 1px solid #e9ecef;
    box-shadow: 0 4px 15px rgba(0, 0, 0, 0.05);
  }

  html[data-theme='dark'] .publications-content {
    background: linear-gradient(135deg, var(--global-card-bg-color) 0%, #2a2a2a 100%);
    border: 1px solid #333;
    box-shadow: 0 4px 15px rgba(0, 0, 0, 0.3);
  }

  .year-line {
    flex: 1;
    height: 1px;
    background: linear-gradient(90deg, #667eea 0%, transparent 100%);
    margin-left: 2rem;
  }

  .publications-content {
    margin-left: 100px;
    padding: 1.5rem;
    background: linear-gradient(135deg, #f8f9fa 0%, #ffffff 100%);
    border-radius: 12px;
    border: 1px solid #e9ecef;
    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.06);
    transition: all 0.3s ease;
  }

  .publications-content:hover {
    box-shadow: 0 8px 25px rgba(0, 0, 0, 0.1);
    transform: translateY(-2px);
  }

  html[data-theme='dark'] .publications-content:hover {
    box-shadow: 0 8px 25px rgba(0, 0, 0, 0.4);
  }

  /* Style bibliography entries */
  .publications-content .bibliography {
    margin: 0;
  }

  .publications-content .bibliography li {
    margin-bottom: 1.5rem;
    padding: 1rem;
    background: white;
    border-radius: 8px;
    border: 1px solid #f0f0f0;
    transition: all 0.3s ease;
    list-style: none;
  }

  html[data-theme='dark'] .publications-content .bibliography li {
    background: var(--global-bg-color);
    border: 1px solid #444;
  }

  .publications-content .bibliography li:hover {
    border-color: #667eea;
    box-shadow: 0 4px 12px rgba(102, 126, 234, 0.1);
  }

  html[data-theme='dark'] .publications-content .bibliography li:hover {
    border-color: var(--global-theme-color);
    box-shadow: 0 4px 12px rgba(0, 0, 0, 0.3);
  }

  .publications-content .bibliography li:last-child {
    margin-bottom: 0;
  }

  /* Enhanced typography for publication entries */
  .publications-content .title {
    font-weight: 600;
    color: #2c3e50;
    font-size: 1.1rem;
    line-height: 1.4;
    margin-bottom: 0.5rem;
  }

  html[data-theme='dark'] .publications-content .title {
    color: var(--global-text-color);
  }

  .publications-content .author {
    color: #495057;
    margin-bottom: 0.25rem;
  }

  html[data-theme='dark'] .publications-content .author {
    color: var(--global-text-color);
  }

  .publications-content .periodical {
    color: #667eea;
    font-style: italic;
    font-weight: 500;
  }

  html[data-theme='dark'] .publications-content .periodical {
    color: var(--global-theme-color);
  }

  @media (max-width: 768px) {
    .publications-container {
      padding: 1rem;
    }

    .publications-timeline::before {
      left: 20px;
    }

    .year-title {
      font-size: 1.25rem;
      padding: 0.5rem 1rem;
    }

    .year-title::before {
      left: -35px;
    }

    .publications-content {
      margin-left: 60px;
    }

    .year-line {
      display: none;
    }
  }
</style>


  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2025 Mohit  Vaishnav.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    Last updated: October 26, 2025.
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>


</html>
